{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\mohd sufyan\\appdata\\roaming\\python\\python310\\site-packages (3.7.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.3.0)\n",
      "Requirement already satisfied: xgboost in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2022.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (4.38.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (21.3)\n",
      "Requirement already satisfied: pyparsing<3.1,>=2.3.1 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (1.0.5)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from matplotlib) (9.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (3.2.0)\n",
      "Requirement already satisfied: scipy>=1.5.0 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.9.1)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Collecting numpy\n",
      "  Downloading numpy-1.24.4-cp310-cp310-win_amd64.whl (14.8 MB)\n",
      "     ---------------------------------------- 14.8/14.8 MB 4.4 MB/s eta 0:00:00\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.1\n",
      "    Uninstalling numpy-1.26.1:\n",
      "      Successfully uninstalled numpy-1.26.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "    WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Users\\\\MOHD SUFYAN\\\\AppData\\\\Local\\\\Programs\\\\Python\\\\Python310\\\\Lib\\\\site-packages\\\\~umpy.libs\\\\libopenblas64__v0.3.23-293-gc2f4bdbb-gcc_10_3_0-2bde3a66a51006b2b53eb373ff767a3f.dll'\n",
      "Consider using the `--user` option or check the permissions.\n",
      "\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -atplotlib (c:\\users\\mohd sufyan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages)\n",
      "WARNING: There was an error checking the latest version of pip.\n"
     ]
    }
   ],
   "source": [
    "%pip install pandas numpy seaborn matplotlib scikit-learn xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import os\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, classification_report, f1_score, roc_auc_score\n",
    "from sklearn.preprocessing import MinMaxScaler as Scaler\n",
    "# import hyperopt\n",
    "\n",
    "from models import decision_tree, k_nearest, logistic_regression, random_forest, support_vector_cls, xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILENAME = 'pyradiomics_extraction_segmentation_no_maskcorrect_with_FS'\n",
    "\n",
    "RESULTS_PATH = f'Results without preprocessing/{FILENAME}.csv'\n",
    "CLASS_LABELS = '../../Data/Patient class labels.csv'\n",
    "DF_PATH = '../../Data/Without Demographic Features/pyradiomics_extraction_segmentation_no_maskcorrect.csv'\n",
    "MODEL_PICKLING = f'Saved Models/{FILENAME}.pkl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_RATIO = 0.6\n",
    "VAL_RATIO = 0.2\n",
    "TEST_RATIO = 0.2\n",
    "\n",
    "SEED = 2454259\n",
    "\n",
    "FEATURE_SCALE = True\n",
    "CRITERION = 'accuracy'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = pd.read_csv(CLASS_LABELS)\n",
    "features = pd.read_csv(DF_PATH).drop(columns = 'sequence', errors='ignore')\n",
    "\n",
    "total_features = pd.merge(features, labels, left_on = 'patient', right_on = 'Patient ID').drop(columns = ['Patient ID', 'patient'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_val_splits(df, train_ratio = 0.8, val_ratio = 0.2, random_state = 2454259):\n",
    "   \n",
    "    val_ratio_adj = val_ratio / (1-train_ratio)\n",
    "\n",
    "    train_df, val_df = train_test_split(df, train_size = train_ratio, random_state= random_state)\n",
    "    val_df, test_df = train_test_split(val_df, train_size = val_ratio_adj, random_state= random_state)\n",
    "\n",
    "    return train_df, val_df, test_df\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = train_test_val_splits(total_features, TRAIN_RATIO, VAL_RATIO, random_state = SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x= train_df.drop(columns = ['ER', 'PR', 'HER2', 'Mol Subtype'])\n",
    "train_y_er = train_df['ER'].to_numpy()\n",
    "train_y_pr = train_df['PR'].to_numpy()\n",
    "train_y_her = train_df['HER2'].to_numpy()\n",
    "train_y_mol_subtype = train_df['Mol Subtype'].to_numpy()\n",
    "\n",
    "\n",
    "val_x = val_df.drop(columns = ['ER', 'PR', 'HER2', 'Mol Subtype'])\n",
    "val_y_er = val_df['ER'].to_numpy()\n",
    "val_y_pr = val_df['PR'].to_numpy()\n",
    "val_y_her = val_df['HER2'].to_numpy()\n",
    "val_y_mol_subtype = val_df['Mol Subtype'].to_numpy()\n",
    "\n",
    "test_x = test_df.drop(columns = ['ER', 'PR', 'HER2', 'Mol Subtype'])\n",
    "test_y_er = test_df['ER'].to_numpy()\n",
    "test_y_pr = test_df['PR'].to_numpy()\n",
    "test_y_her = test_df['HER2'].to_numpy()\n",
    "test_y_mol_subtype = test_df['Mol Subtype'].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features scaled\n"
     ]
    }
   ],
   "source": [
    "if FEATURE_SCALE == True:\n",
    "    scaler = Scaler()\n",
    "    scaler.fit(train_x)\n",
    "    train_x = scaler.transform(train_x)\n",
    "    val_x = scaler.transform(val_x)\n",
    "    test_x = scaler.transform(test_x)\n",
    "    print('Features scaled')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifications_classes = {\n",
    "    'ER' : (train_x, train_y_er, val_x, val_y_er, test_x, test_y_er),\n",
    "    'PR' : (train_x, train_y_pr, val_x, val_y_pr, test_x, test_y_pr),\n",
    "    'HER2': (train_x, train_y_her, val_x, val_y_her, test_x, test_y_her),\n",
    "    'Mol Subtype': (train_x, train_y_mol_subtype, val_x, val_y_mol_subtype, test_x, test_y_mol_subtype)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(model, x, truey, split):\n",
    "    pred_y = model.predict(x)\n",
    "    acc = accuracy_score(truey, pred_y)\n",
    "    prec = precision_score(truey, pred_y, average = 'weighted')\n",
    "    rec = recall_score(truey, pred_y, average = 'weighted')\n",
    "    f1 = f1_score(truey, pred_y, average = 'weighted')\n",
    "\n",
    "    return {f'{split}_acc': acc, f'{split}_prec': prec, f'{split}_rec': rec, f'{split}_f1': f1}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_NB_model(tx, ty, vx, vy, testx, testy, classification = None):\n",
    "    nb = GaussianNB()\n",
    "    nb.fit(tx, ty)\n",
    "    \n",
    "    train_metrics = get_metrics(nb, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(nb, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(nb, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **test_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'Naive Bayes'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, nb\n",
    "\n",
    "def get_DT_model(tx, ty, vx, vy, testx, testy, criterion = 'accuracy', classification = None):\n",
    "    '''Get Decision Tree model with metrics'''\n",
    "    model = decision_tree.get_best_hyperparameters(tx, ty, vx, vy, criterion = criterion)\n",
    "\n",
    "    train_metrics = get_metrics(model, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(model, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(model, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **test_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'Decision Tree'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, model\n",
    "\n",
    "def get_KNN_model(tx, ty, vx, vy, testx, testy, criterion = 'accuracy', classification = None):\n",
    "    '''Get KNN model with metrics'''\n",
    "    model = k_nearest.get_best_hyperparameters(tx, ty, vx, vy, criterion = criterion)\n",
    "\n",
    "    train_metrics = get_metrics(model, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(model, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(model, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **test_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'KNN'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, model\n",
    "\n",
    "def get_LR_model(tx, ty, vx, vy, testx, testy, criterion = 'accuracy', classification = None):\n",
    "    '''Get Logistic Regression model with metrics'''\n",
    "    model = logistic_regression.get_best_hyperparameters(tx, ty, vx, vy, criterion = criterion)\n",
    "\n",
    "    train_metrics = get_metrics(model, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(model, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(model, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **test_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'Logistic Regression'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, model\n",
    "\n",
    "def get_RF_model(tx, ty, vx, vy, testx, testy, criterion = 'accuracy', classification = None):\n",
    "    '''Get Random forest model with metrics'''\n",
    "    model = random_forest.get_best_hyperparameters(tx, ty, vx, vy, criterion = criterion)\n",
    "\n",
    "    train_metrics = get_metrics(model, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(model, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(model, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **test_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'Random Forest'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, model\n",
    "\n",
    "def get_SVM_model(tx, ty, vx, vy, testx, testy, criterion = 'accuracy', classification = None):\n",
    "    '''Get SVM model with metrics'''\n",
    "    model = support_vector_cls.get_best_hyperparameters(tx, ty, vx, vy, criterion = criterion)\n",
    "\n",
    "    train_metrics = get_metrics(model, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(model, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(model, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **test_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'SVM'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, model\n",
    "    \n",
    "def get_XGB_model(tx, ty, vx, vy, testx, testy, criterion = 'accuracy', classification = None):\n",
    "    '''Get XGBoost model with metrics'''\n",
    "    model = xgboost.get_best_hyperparameters(tx, ty, vx, vy, criterion = criterion)\n",
    "\n",
    "    train_metrics = get_metrics(model, tx, ty, 'train')\n",
    "    val_metrics = get_metrics(model, vx, vy, 'val')\n",
    "    test_metrics = get_metrics(model, testx, testy, 'test')\n",
    "\n",
    "    combined_metrics = {**train_metrics, **val_metrics, **train_metrics}\n",
    "\n",
    "    result = pd.DataFrame(combined_metrics, index = [0])\n",
    "    result['Algorithm'] = 'XGBoost'\n",
    "    if classification is not None:\n",
    "        result['classification'] = classification\n",
    "\n",
    "    return result, model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:350: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1469: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "classification_results = pd.DataFrame()\n",
    "models = {}\n",
    "\n",
    "for key, value in classifications_classes.items():\n",
    "\n",
    "    tx, ty, vx, vy, testx, testy = value\n",
    "    \n",
    "    res_nb, nb = get_NB_model(tx, ty, vx, vy, testx, testy, classification = key)\n",
    "    res_dt, dt = get_DT_model(tx, ty, vx, vy, testx, testy, criterion = CRITERION, classification = key)\n",
    "    res_knn, knn = get_KNN_model(tx, ty, vx, vy, testx, testy, criterion = CRITERION, classification = key)\n",
    "    res_lr, lr = get_LR_model(tx, ty, vx, vy, testx, testy, criterion = CRITERION, classification = key)\n",
    "    res_rf, rf = get_RF_model(tx, ty, vx, vy, testx, testy, criterion = CRITERION, classification = key)\n",
    "    res_svm, svm = get_SVM_model(tx, ty, vx, vy, testx, testy, criterion = CRITERION, classification = key)\n",
    "    res_xgb, xgb = get_XGB_model(tx, ty, vx, vy, testx, testy, criterion = CRITERION, classification = key)\n",
    "\n",
    "    models[key] = {'Naive Bayes' : nb,\n",
    "                   'Decision Trees' : dt,\n",
    "                   'K Nearest Neighbours': knn,\n",
    "                   'Logistic Regression' : lr,\n",
    "                   'Random Forest' : rf,\n",
    "                   'SVM': svm,\n",
    "                   'XGBoost': xgb}\n",
    "\n",
    "    classification_results = pd.concat([classification_results,\n",
    "                                        res_nb,\n",
    "                                        res_dt,\n",
    "                                        res_knn,\n",
    "                                        res_lr,\n",
    "                                        res_rf,\n",
    "                                        res_svm,\n",
    "                                        res_xgb])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>train_acc</th>\n",
       "      <th>train_prec</th>\n",
       "      <th>train_rec</th>\n",
       "      <th>train_f1</th>\n",
       "      <th>val_acc</th>\n",
       "      <th>val_prec</th>\n",
       "      <th>val_rec</th>\n",
       "      <th>val_f1</th>\n",
       "      <th>test_acc</th>\n",
       "      <th>test_prec</th>\n",
       "      <th>test_rec</th>\n",
       "      <th>test_f1</th>\n",
       "      <th>Algorithm</th>\n",
       "      <th>classification</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.273056</td>\n",
       "      <td>0.719190</td>\n",
       "      <td>0.273056</td>\n",
       "      <td>0.150630</td>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.812842</td>\n",
       "      <td>0.255435</td>\n",
       "      <td>0.111228</td>\n",
       "      <td>0.275676</td>\n",
       "      <td>0.436086</td>\n",
       "      <td>0.275676</td>\n",
       "      <td>0.146107</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.753257</td>\n",
       "      <td>0.772152</td>\n",
       "      <td>0.757377</td>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.776936</td>\n",
       "      <td>0.793478</td>\n",
       "      <td>0.765294</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.673259</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.681840</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.819168</td>\n",
       "      <td>0.808564</td>\n",
       "      <td>0.819168</td>\n",
       "      <td>0.803707</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.790707</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.791512</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>0.651666</td>\n",
       "      <td>0.681081</td>\n",
       "      <td>0.662040</td>\n",
       "      <td>KNN</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.764919</td>\n",
       "      <td>0.753688</td>\n",
       "      <td>0.764919</td>\n",
       "      <td>0.692894</td>\n",
       "      <td>0.766304</td>\n",
       "      <td>0.774022</td>\n",
       "      <td>0.766304</td>\n",
       "      <td>0.687481</td>\n",
       "      <td>0.713514</td>\n",
       "      <td>0.617937</td>\n",
       "      <td>0.713514</td>\n",
       "      <td>0.621021</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.718919</td>\n",
       "      <td>0.665501</td>\n",
       "      <td>0.718919</td>\n",
       "      <td>0.658903</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.817360</td>\n",
       "      <td>0.842990</td>\n",
       "      <td>0.817360</td>\n",
       "      <td>0.777997</td>\n",
       "      <td>0.842391</td>\n",
       "      <td>0.869760</td>\n",
       "      <td>0.842391</td>\n",
       "      <td>0.813609</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.634835</td>\n",
       "      <td>0.702703</td>\n",
       "      <td>0.642221</td>\n",
       "      <td>SVM</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.904492</td>\n",
       "      <td>0.896926</td>\n",
       "      <td>0.888337</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.905063</td>\n",
       "      <td>0.891304</td>\n",
       "      <td>0.879880</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>ER</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.365280</td>\n",
       "      <td>0.665492</td>\n",
       "      <td>0.365280</td>\n",
       "      <td>0.218279</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.770670</td>\n",
       "      <td>0.364130</td>\n",
       "      <td>0.200930</td>\n",
       "      <td>0.345946</td>\n",
       "      <td>0.444919</td>\n",
       "      <td>0.345946</td>\n",
       "      <td>0.204585</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.679928</td>\n",
       "      <td>0.785623</td>\n",
       "      <td>0.679928</td>\n",
       "      <td>0.577329</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.783830</td>\n",
       "      <td>0.673913</td>\n",
       "      <td>0.571093</td>\n",
       "      <td>0.637838</td>\n",
       "      <td>0.554317</td>\n",
       "      <td>0.637838</td>\n",
       "      <td>0.542407</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.773960</td>\n",
       "      <td>0.769008</td>\n",
       "      <td>0.773960</td>\n",
       "      <td>0.768824</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.755637</td>\n",
       "      <td>0.760870</td>\n",
       "      <td>0.753380</td>\n",
       "      <td>0.578378</td>\n",
       "      <td>0.555732</td>\n",
       "      <td>0.578378</td>\n",
       "      <td>0.564018</td>\n",
       "      <td>KNN</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.609164</td>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.563704</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.627475</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.570531</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.635791</td>\n",
       "      <td>0.664865</td>\n",
       "      <td>0.592060</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.840868</td>\n",
       "      <td>0.867594</td>\n",
       "      <td>0.840868</td>\n",
       "      <td>0.827216</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.841122</td>\n",
       "      <td>0.804348</td>\n",
       "      <td>0.782727</td>\n",
       "      <td>0.643243</td>\n",
       "      <td>0.602208</td>\n",
       "      <td>0.643243</td>\n",
       "      <td>0.596542</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.776364</td>\n",
       "      <td>0.759494</td>\n",
       "      <td>0.731120</td>\n",
       "      <td>0.777174</td>\n",
       "      <td>0.806009</td>\n",
       "      <td>0.777174</td>\n",
       "      <td>0.751362</td>\n",
       "      <td>0.686486</td>\n",
       "      <td>0.667515</td>\n",
       "      <td>0.686486</td>\n",
       "      <td>0.649098</td>\n",
       "      <td>SVM</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.956600</td>\n",
       "      <td>0.959320</td>\n",
       "      <td>0.956600</td>\n",
       "      <td>0.955871</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.949898</td>\n",
       "      <td>0.945652</td>\n",
       "      <td>0.944529</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>PR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.197107</td>\n",
       "      <td>0.769259</td>\n",
       "      <td>0.197107</td>\n",
       "      <td>0.088566</td>\n",
       "      <td>0.168478</td>\n",
       "      <td>0.028385</td>\n",
       "      <td>0.168478</td>\n",
       "      <td>0.048584</td>\n",
       "      <td>0.178378</td>\n",
       "      <td>0.652098</td>\n",
       "      <td>0.178378</td>\n",
       "      <td>0.079036</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.849910</td>\n",
       "      <td>0.873195</td>\n",
       "      <td>0.849910</td>\n",
       "      <td>0.803878</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.840879</td>\n",
       "      <td>0.847826</td>\n",
       "      <td>0.798833</td>\n",
       "      <td>0.821622</td>\n",
       "      <td>0.736352</td>\n",
       "      <td>0.821622</td>\n",
       "      <td>0.760007</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.804105</td>\n",
       "      <td>0.822785</td>\n",
       "      <td>0.754009</td>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.691428</td>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.755032</td>\n",
       "      <td>0.827027</td>\n",
       "      <td>0.692186</td>\n",
       "      <td>0.827027</td>\n",
       "      <td>0.753622</td>\n",
       "      <td>KNN</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.817360</td>\n",
       "      <td>0.668077</td>\n",
       "      <td>0.817360</td>\n",
       "      <td>0.735217</td>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.691428</td>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.755032</td>\n",
       "      <td>0.827027</td>\n",
       "      <td>0.692186</td>\n",
       "      <td>0.827027</td>\n",
       "      <td>0.753622</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.842676</td>\n",
       "      <td>0.868070</td>\n",
       "      <td>0.842676</td>\n",
       "      <td>0.790073</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.879220</td>\n",
       "      <td>0.858696</td>\n",
       "      <td>0.813202</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.692944</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.756310</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.817360</td>\n",
       "      <td>0.668077</td>\n",
       "      <td>0.817360</td>\n",
       "      <td>0.735217</td>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.691428</td>\n",
       "      <td>0.831522</td>\n",
       "      <td>0.755032</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.692944</td>\n",
       "      <td>0.832432</td>\n",
       "      <td>0.756310</td>\n",
       "      <td>SVM</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.853526</td>\n",
       "      <td>0.875786</td>\n",
       "      <td>0.853526</td>\n",
       "      <td>0.810522</td>\n",
       "      <td>0.864130</td>\n",
       "      <td>0.883213</td>\n",
       "      <td>0.864130</td>\n",
       "      <td>0.823360</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>HER2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.179024</td>\n",
       "      <td>0.618714</td>\n",
       "      <td>0.179024</td>\n",
       "      <td>0.075820</td>\n",
       "      <td>0.168478</td>\n",
       "      <td>0.028540</td>\n",
       "      <td>0.168478</td>\n",
       "      <td>0.048811</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.050084</td>\n",
       "      <td>0.216216</td>\n",
       "      <td>0.081329</td>\n",
       "      <td>Naive Bayes</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.678119</td>\n",
       "      <td>0.675592</td>\n",
       "      <td>0.678119</td>\n",
       "      <td>0.585747</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.605116</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.608583</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.474559</td>\n",
       "      <td>0.600000</td>\n",
       "      <td>0.488863</td>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.652803</td>\n",
       "      <td>0.534972</td>\n",
       "      <td>0.652803</td>\n",
       "      <td>0.521761</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.490166</td>\n",
       "      <td>0.652174</td>\n",
       "      <td>0.541073</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.375118</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.464792</td>\n",
       "      <td>KNN</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.423794</td>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.513380</td>\n",
       "      <td>0.657609</td>\n",
       "      <td>0.438406</td>\n",
       "      <td>0.657609</td>\n",
       "      <td>0.526087</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.373090</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.463232</td>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.687161</td>\n",
       "      <td>0.788702</td>\n",
       "      <td>0.687161</td>\n",
       "      <td>0.589493</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.688141</td>\n",
       "      <td>0.695652</td>\n",
       "      <td>0.596108</td>\n",
       "      <td>0.605405</td>\n",
       "      <td>0.371798</td>\n",
       "      <td>0.605405</td>\n",
       "      <td>0.460679</td>\n",
       "      <td>Random Forest</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.423794</td>\n",
       "      <td>0.650995</td>\n",
       "      <td>0.513380</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.439627</td>\n",
       "      <td>0.663043</td>\n",
       "      <td>0.528701</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.373090</td>\n",
       "      <td>0.610811</td>\n",
       "      <td>0.463232</td>\n",
       "      <td>SVM</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.735986</td>\n",
       "      <td>0.800692</td>\n",
       "      <td>0.735986</td>\n",
       "      <td>0.678034</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.812788</td>\n",
       "      <td>0.739130</td>\n",
       "      <td>0.674153</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>XGBoost</td>\n",
       "      <td>Mol Subtype</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   train_acc  train_prec  train_rec  train_f1   val_acc  val_prec   val_rec  \\\n",
       "0   0.273056    0.719190   0.273056  0.150630  0.255435  0.812842  0.255435   \n",
       "0   0.772152    0.753257   0.772152  0.757377  0.793478  0.776936  0.793478   \n",
       "0   0.819168    0.808564   0.819168  0.803707  0.804348  0.790707  0.804348   \n",
       "0   0.764919    0.753688   0.764919  0.692894  0.766304  0.774022  0.766304   \n",
       "0   1.000000    1.000000   1.000000  1.000000  1.000000  1.000000  1.000000   \n",
       "0   0.817360    0.842990   0.817360  0.777997  0.842391  0.869760  0.842391   \n",
       "0   0.896926    0.904492   0.896926  0.888337  0.891304  0.905063  0.891304   \n",
       "0   0.365280    0.665492   0.365280  0.218279  0.364130  0.770670  0.364130   \n",
       "0   0.679928    0.785623   0.679928  0.577329  0.673913  0.783830  0.673913   \n",
       "0   0.773960    0.769008   0.773960  0.768824  0.760870  0.755637  0.760870   \n",
       "0   0.650995    0.609164   0.650995  0.563704  0.652174  0.627475  0.652174   \n",
       "0   0.840868    0.867594   0.840868  0.827216  0.804348  0.841122  0.804348   \n",
       "0   0.759494    0.776364   0.759494  0.731120  0.777174  0.806009  0.777174   \n",
       "0   0.956600    0.959320   0.956600  0.955871  0.945652  0.949898  0.945652   \n",
       "0   0.197107    0.769259   0.197107  0.088566  0.168478  0.028385  0.168478   \n",
       "0   0.849910    0.873195   0.849910  0.803878  0.847826  0.840879  0.847826   \n",
       "0   0.822785    0.804105   0.822785  0.754009  0.831522  0.691428  0.831522   \n",
       "0   0.817360    0.668077   0.817360  0.735217  0.831522  0.691428  0.831522   \n",
       "0   0.842676    0.868070   0.842676  0.790073  0.858696  0.879220  0.858696   \n",
       "0   0.817360    0.668077   0.817360  0.735217  0.831522  0.691428  0.831522   \n",
       "0   0.853526    0.875786   0.853526  0.810522  0.864130  0.883213  0.864130   \n",
       "0   0.179024    0.618714   0.179024  0.075820  0.168478  0.028540  0.168478   \n",
       "0   0.678119    0.675592   0.678119  0.585747  0.695652  0.605116  0.695652   \n",
       "0   0.652803    0.534972   0.652803  0.521761  0.652174  0.490166  0.652174   \n",
       "0   0.650995    0.423794   0.650995  0.513380  0.657609  0.438406  0.657609   \n",
       "0   0.687161    0.788702   0.687161  0.589493  0.695652  0.688141  0.695652   \n",
       "0   0.650995    0.423794   0.650995  0.513380  0.663043  0.439627  0.663043   \n",
       "0   0.735986    0.800692   0.735986  0.678034  0.739130  0.812788  0.739130   \n",
       "\n",
       "     val_f1  test_acc  test_prec  test_rec   test_f1            Algorithm  \\\n",
       "0  0.111228  0.275676   0.436086  0.275676  0.146107          Naive Bayes   \n",
       "0  0.765294  0.702703   0.673259  0.702703  0.681840        Decision Tree   \n",
       "0  0.791512  0.681081   0.651666  0.681081  0.662040                  KNN   \n",
       "0  0.687481  0.713514   0.617937  0.713514  0.621021  Logistic Regression   \n",
       "0  1.000000  0.718919   0.665501  0.718919  0.658903        Random Forest   \n",
       "0  0.813609  0.702703   0.634835  0.702703  0.642221                  SVM   \n",
       "0  0.879880       NaN        NaN       NaN       NaN              XGBoost   \n",
       "0  0.200930  0.345946   0.444919  0.345946  0.204585          Naive Bayes   \n",
       "0  0.571093  0.637838   0.554317  0.637838  0.542407        Decision Tree   \n",
       "0  0.753380  0.578378   0.555732  0.578378  0.564018                  KNN   \n",
       "0  0.570531  0.664865   0.635791  0.664865  0.592060  Logistic Regression   \n",
       "0  0.782727  0.643243   0.602208  0.643243  0.596542        Random Forest   \n",
       "0  0.751362  0.686486   0.667515  0.686486  0.649098                  SVM   \n",
       "0  0.944529       NaN        NaN       NaN       NaN              XGBoost   \n",
       "0  0.048584  0.178378   0.652098  0.178378  0.079036          Naive Bayes   \n",
       "0  0.798833  0.821622   0.736352  0.821622  0.760007        Decision Tree   \n",
       "0  0.755032  0.827027   0.692186  0.827027  0.753622                  KNN   \n",
       "0  0.755032  0.827027   0.692186  0.827027  0.753622  Logistic Regression   \n",
       "0  0.813202  0.832432   0.692944  0.832432  0.756310        Random Forest   \n",
       "0  0.755032  0.832432   0.692944  0.832432  0.756310                  SVM   \n",
       "0  0.823360       NaN        NaN       NaN       NaN              XGBoost   \n",
       "0  0.048811  0.216216   0.050084  0.216216  0.081329          Naive Bayes   \n",
       "0  0.608583  0.600000   0.474559  0.600000  0.488863        Decision Tree   \n",
       "0  0.541073  0.610811   0.375118  0.610811  0.464792                  KNN   \n",
       "0  0.526087  0.610811   0.373090  0.610811  0.463232  Logistic Regression   \n",
       "0  0.596108  0.605405   0.371798  0.605405  0.460679        Random Forest   \n",
       "0  0.528701  0.610811   0.373090  0.610811  0.463232                  SVM   \n",
       "0  0.674153       NaN        NaN       NaN       NaN              XGBoost   \n",
       "\n",
       "  classification  \n",
       "0             ER  \n",
       "0             ER  \n",
       "0             ER  \n",
       "0             ER  \n",
       "0             ER  \n",
       "0             ER  \n",
       "0             ER  \n",
       "0             PR  \n",
       "0             PR  \n",
       "0             PR  \n",
       "0             PR  \n",
       "0             PR  \n",
       "0             PR  \n",
       "0             PR  \n",
       "0           HER2  \n",
       "0           HER2  \n",
       "0           HER2  \n",
       "0           HER2  \n",
       "0           HER2  \n",
       "0           HER2  \n",
       "0           HER2  \n",
       "0    Mol Subtype  \n",
       "0    Mol Subtype  \n",
       "0    Mol Subtype  \n",
       "0    Mol Subtype  \n",
       "0    Mol Subtype  \n",
       "0    Mol Subtype  \n",
       "0    Mol Subtype  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classification_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Cannot save file into a non-existent directory: 'Results without preprocessing'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32me:\\ML Project\\New folder\\Breast-Cancer-Subtype-from-MRI\\4.1. ML Modelling\\ML Models Feature Scaling\\[Unified Script] Results Creation.ipynb Cell 16\u001b[0m line \u001b[0;36m7\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML%20Project/New%20folder/Breast-Cancer-Subtype-from-MRI/4.1.%20ML%20Modelling/ML%20Models%20Feature%20Scaling/%5BUnified%20Script%5D%20Results%20Creation.ipynb#X21sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m cols \u001b[39m=\u001b[39m cols[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39m+\u001b[39m cols[:\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m]\n\u001b[0;32m      <a href='vscode-notebook-cell:/e%3A/ML%20Project/New%20folder/Breast-Cancer-Subtype-from-MRI/4.1.%20ML%20Modelling/ML%20Models%20Feature%20Scaling/%5BUnified%20Script%5D%20Results%20Creation.ipynb#X21sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m classification_results \u001b[39m=\u001b[39m classification_results[cols]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/e%3A/ML%20Project/New%20folder/Breast-Cancer-Subtype-from-MRI/4.1.%20ML%20Modelling/ML%20Models%20Feature%20Scaling/%5BUnified%20Script%5D%20Results%20Creation.ipynb#X21sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m classification_results\u001b[39m.\u001b[39;49mto_csv(RESULTS_PATH, index \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20Project/New%20folder/Breast-Cancer-Subtype-from-MRI/4.1.%20ML%20Modelling/ML%20Models%20Feature%20Scaling/%5BUnified%20Script%5D%20Results%20Creation.ipynb#X21sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(MODEL_PICKLING, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m) \u001b[39mas\u001b[39;00m file:\n\u001b[0;32m     <a href='vscode-notebook-cell:/e%3A/ML%20Project/New%20folder/Breast-Cancer-Subtype-from-MRI/4.1.%20ML%20Modelling/ML%20Models%20Feature%20Scaling/%5BUnified%20Script%5D%20Results%20Creation.ipynb#X21sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     pkl\u001b[39m.\u001b[39mdump(models, file)\n",
      "File \u001b[1;32mc:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\core\\generic.py:3551\u001b[0m, in \u001b[0;36mNDFrame.to_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors, storage_options)\u001b[0m\n\u001b[0;32m   3540\u001b[0m df \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m, ABCDataFrame) \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mto_frame()\n\u001b[0;32m   3542\u001b[0m formatter \u001b[39m=\u001b[39m DataFrameFormatter(\n\u001b[0;32m   3543\u001b[0m     frame\u001b[39m=\u001b[39mdf,\n\u001b[0;32m   3544\u001b[0m     header\u001b[39m=\u001b[39mheader,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3548\u001b[0m     decimal\u001b[39m=\u001b[39mdecimal,\n\u001b[0;32m   3549\u001b[0m )\n\u001b[1;32m-> 3551\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrameRenderer(formatter)\u001b[39m.\u001b[39;49mto_csv(\n\u001b[0;32m   3552\u001b[0m     path_or_buf,\n\u001b[0;32m   3553\u001b[0m     line_terminator\u001b[39m=\u001b[39;49mline_terminator,\n\u001b[0;32m   3554\u001b[0m     sep\u001b[39m=\u001b[39;49msep,\n\u001b[0;32m   3555\u001b[0m     encoding\u001b[39m=\u001b[39;49mencoding,\n\u001b[0;32m   3556\u001b[0m     errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m   3557\u001b[0m     compression\u001b[39m=\u001b[39;49mcompression,\n\u001b[0;32m   3558\u001b[0m     quoting\u001b[39m=\u001b[39;49mquoting,\n\u001b[0;32m   3559\u001b[0m     columns\u001b[39m=\u001b[39;49mcolumns,\n\u001b[0;32m   3560\u001b[0m     index_label\u001b[39m=\u001b[39;49mindex_label,\n\u001b[0;32m   3561\u001b[0m     mode\u001b[39m=\u001b[39;49mmode,\n\u001b[0;32m   3562\u001b[0m     chunksize\u001b[39m=\u001b[39;49mchunksize,\n\u001b[0;32m   3563\u001b[0m     quotechar\u001b[39m=\u001b[39;49mquotechar,\n\u001b[0;32m   3564\u001b[0m     date_format\u001b[39m=\u001b[39;49mdate_format,\n\u001b[0;32m   3565\u001b[0m     doublequote\u001b[39m=\u001b[39;49mdoublequote,\n\u001b[0;32m   3566\u001b[0m     escapechar\u001b[39m=\u001b[39;49mescapechar,\n\u001b[0;32m   3567\u001b[0m     storage_options\u001b[39m=\u001b[39;49mstorage_options,\n\u001b[0;32m   3568\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\format.py:1180\u001b[0m, in \u001b[0;36mDataFrameRenderer.to_csv\u001b[1;34m(self, path_or_buf, encoding, sep, columns, index_label, mode, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, errors, storage_options)\u001b[0m\n\u001b[0;32m   1159\u001b[0m     created_buffer \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m   1161\u001b[0m csv_formatter \u001b[39m=\u001b[39m CSVFormatter(\n\u001b[0;32m   1162\u001b[0m     path_or_buf\u001b[39m=\u001b[39mpath_or_buf,\n\u001b[0;32m   1163\u001b[0m     line_terminator\u001b[39m=\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1178\u001b[0m     formatter\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfmt,\n\u001b[0;32m   1179\u001b[0m )\n\u001b[1;32m-> 1180\u001b[0m csv_formatter\u001b[39m.\u001b[39;49msave()\n\u001b[0;32m   1182\u001b[0m \u001b[39mif\u001b[39;00m created_buffer:\n\u001b[0;32m   1183\u001b[0m     \u001b[39massert\u001b[39;00m \u001b[39misinstance\u001b[39m(path_or_buf, StringIO)\n",
      "File \u001b[1;32mc:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\formats\\csvs.py:241\u001b[0m, in \u001b[0;36mCSVFormatter.save\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    237\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    238\u001b[0m \u001b[39mCreate the writer & save.\u001b[39;00m\n\u001b[0;32m    239\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    240\u001b[0m \u001b[39m# apply compression and byte/text conversion\u001b[39;00m\n\u001b[1;32m--> 241\u001b[0m \u001b[39mwith\u001b[39;00m get_handle(\n\u001b[0;32m    242\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilepath_or_buffer,\n\u001b[0;32m    243\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    244\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    245\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49merrors,\n\u001b[0;32m    246\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompression,\n\u001b[0;32m    247\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstorage_options,\n\u001b[0;32m    248\u001b[0m ) \u001b[39mas\u001b[39;00m handles:\n\u001b[0;32m    249\u001b[0m \n\u001b[0;32m    250\u001b[0m     \u001b[39m# Note: self.encoding is irrelevant here\u001b[39;00m\n\u001b[0;32m    251\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwriter \u001b[39m=\u001b[39m csvlib\u001b[39m.\u001b[39mwriter(\n\u001b[0;32m    252\u001b[0m         handles\u001b[39m.\u001b[39mhandle,\n\u001b[0;32m    253\u001b[0m         lineterminator\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mline_terminator,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m         quotechar\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mquotechar,\n\u001b[0;32m    259\u001b[0m     )\n\u001b[0;32m    261\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_save()\n",
      "File \u001b[1;32mc:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:697\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[39m# Only for write methods\u001b[39;00m\n\u001b[0;32m    696\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode \u001b[39mand\u001b[39;00m is_path:\n\u001b[1;32m--> 697\u001b[0m     check_parent_directory(\u001b[39mstr\u001b[39;49m(handle))\n\u001b[0;32m    699\u001b[0m \u001b[39mif\u001b[39;00m compression:\n\u001b[0;32m    700\u001b[0m     \u001b[39mif\u001b[39;00m compression \u001b[39m!=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mzstd\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m    701\u001b[0m         \u001b[39m# compression libraries do not like an explicit text-mode\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\MOHD SUFYAN\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pandas\\io\\common.py:571\u001b[0m, in \u001b[0;36mcheck_parent_directory\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    569\u001b[0m parent \u001b[39m=\u001b[39m Path(path)\u001b[39m.\u001b[39mparent\n\u001b[0;32m    570\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m parent\u001b[39m.\u001b[39mis_dir():\n\u001b[1;32m--> 571\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mOSError\u001b[39;00m(\u001b[39mrf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCannot save file into a non-existent directory: \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{\u001b[39;00mparent\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mOSError\u001b[0m: Cannot save file into a non-existent directory: 'Results without preprocessing'"
     ]
    }
   ],
   "source": [
    "classification_results\n",
    "cols = classification_results.columns.tolist()\n",
    "\n",
    "cols = cols[-2:] + cols[:-2]\n",
    "classification_results = classification_results[cols]\n",
    "\n",
    "classification_results.to_csv(RESULTS_PATH, index = False)\n",
    "\n",
    "\n",
    "with open(MODEL_PICKLING, 'wb') as file:\n",
    "    pkl.dump(models, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.16 ('mri_project')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "46a30f71ba13f968ce13e2ff6d1c210f2ffd70fd7dc0256e6c75b999f7c20949"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
